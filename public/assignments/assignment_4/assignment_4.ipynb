{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Assignment 4: N-gram Language Models\n\n## ML Pipeline Overview\n\nThis assignment demonstrates a complete ML pipeline for **n-gram language modeling with smoothing and interpolation**:\n\n### ML Goal\nBuild a probabilistic language model that can:\n1. **Estimate probabilities** of word sequences using n-grams\n2. **Handle unseen n-grams** using smoothing techniques\n3. **Evaluate model quality** using perplexity\n4. **Combine multiple models** using interpolation\n\n### Training Data\nCorpus of sentences used to compute n-gram counts and probabilities\n\n---\n\n### Through-Line Example: \"I am happy\"\n\nLet's trace how we build a language model for the sentence **\"I am happy\"**:\n\n#### **Progression from Assignment 1 to Assignment 4**\n\n| Assignment | Task | Method | Example |\n|------------|------|--------|----------|\n| **A1: Autocomplete** | Complete partial word | Frequency-based prefix matching | \"happ\" \u2192 \"happy\" (most frequent match) |\n| **A3: Spell-check** | Correct misspelled word | Edit distance + frequency | \"happpy\" \u2192 \"happy\" (1 edit, high frequency) |\n| **A4: Language Model** | Predict next word probability | N-gram conditional probability | \"I am\" \u2192 P(happy\\|I am) = 0.6 |\n\n**Key Evolution:** \n- A1: \"What's the most frequent word with this prefix?\"\n- A3: \"What's the most likely correction given edit distance?\"\n- A4: **\"What's the probability of this word given the previous words?\"**\n\n---\n\n### Pipeline Stages\n\n#### **1. N-gram Probability Calculation (Q1)**\n\n**Training Corpus:**\n```\nI am happy\nI am learning\nI am happy because I am learning\n```\n\n**Build N-gram Counts:**\n\n$$\n\\begin{align}\nC(\\text{I am}) &= 3 \\\\\nC(\\text{am happy}) &= 2 \\\\\nC(\\text{am learning}) &= 2\n\\end{align}\n$$\n\n**Calculate probability using MLE (Maximum Likelihood Estimation):**\n\n$$\n\\begin{align}\nP(\\text{happy}|\\text{am}) &= \\frac{C(\\text{am happy})}{C(\\text{am})} = \\frac{2}{4} = 0.5 \\\\\nP(\\text{learning}|\\text{am}) &= \\frac{C(\\text{am learning})}{C(\\text{am})} = \\frac{2}{4} = 0.5\n\\end{align}\n$$\n\n**Apply Chain Rule for Full Sentence:**\n\n$$\n\\begin{align}\nP(\\text{I am happy}) &= P(\\text{I}) \\times P(\\text{am}|\\text{I}) \\times P(\\text{happy}|\\text{am}) \\\\\n&= 1.0 \\times 1.0 \\times 0.5 = 0.5\n\\end{align}\n$$\n\n---\n\n#### **2. Laplace Smoothing (Q2)**\n\n**Problem:** What if we see \"am sad\" in test set, but it never appeared in training?\n\n$$\nP(\\text{sad}|\\text{am}) = \\frac{C(\\text{am sad})}{C(\\text{am})} = \\frac{0}{4} = 0 \\quad \u274c \\text{ Zero probability!}\n$$\n\n**Solution: Add-1 (Laplace) Smoothing**\n\n$$\n\\begin{align}\nP_{\\text{laplace}}(\\text{sad}|\\text{am}) &= \\frac{C(\\text{am sad}) + 1}{C(\\text{am}) + V} \\\\\n&= \\frac{0 + 1}{4 + 10000} \\quad \\text{(V = vocabulary size)} \\\\\n&= 0.0001 \\quad \u2713 \\text{ Small but non-zero!}\n\\end{align}\n$$\n\n**Effect on Known N-grams:**\n\n$$\n\\begin{align}\nP_{\\text{laplace}}(\\text{happy}|\\text{am}) &= \\frac{2 + 1}{4 + 10000} = 0.0003\n\\end{align}\n$$\n\n*Note: All probabilities reduced, but unknown gets non-zero value*\n\n---\n\n#### **3. Perplexity Evaluation (Q3)**\n\n**Goal:** Measure how \"surprised\" the model is by the test set\n\n**Test sentence:** \"I am happy\"\n\n$$\nP(\\text{I am happy}) = 0.5\n$$\n\n**Perplexity formula (inverse geometric mean of probabilities):**\n\n$$\n\\text{PP} = P(\\text{test\\_set})^{-1/M} \\quad \\text{where } M = \\text{number of words}\n$$\n\n$$\n\\text{PP} = 0.5^{-1/3} = 1.26\n$$\n\n**Interpretation:**\n- **Lower perplexity = better model** (less surprised)\n- PP = 1: Perfect prediction (model assigns probability 1)\n- PP = 100: Model is as confused as if choosing randomly from 100 words\n\n---\n\n#### **4. Add-k Smoothing (Q4)**\n\n**Problem:** Laplace (k=1) gives too much probability to unseen n-grams\n\n**Solution: Use smaller k (e.g., k=0.1)**\n\n$$\n\\begin{align}\nP_k(\\text{sad}|\\text{am}) &= \\frac{0 + 0.1}{4 + 0.1 \\times 10000} = 0.00001 \\\\\nP_k(\\text{happy}|\\text{am}) &= \\frac{2 + 0.1}{4 + 0.1 \\times 10000} = 0.00021\n\\end{align}\n$$\n\n*Benefit: Known n-grams keep more of their probability mass*\n\n---\n\n#### **5. Linear Interpolation (Q5)**\n\n**Idea:** Combine trigram, bigram, and unigram probabilities with weights\n\n**Given:** \"I am happy\"\n\n$$\n\\begin{align}\nP_{\\text{trigram}}(\\text{happy}|\\text{I am}) &= 0.8 \\\\\nP_{\\text{bigram}}(\\text{happy}|\\text{am}) &= 0.5 \\\\\nP_{\\text{unigram}}(\\text{happy}) &= 0.3\n\\end{align}\n$$\n\n**Weighted combination** $(\\lambda_1 + \\lambda_2 + \\lambda_3 = 1)$:\n\n$$\n\\begin{align}\nP_{\\text{interpolated}} &= \\lambda_1 \\times P_{\\text{trigram}} + \\lambda_2 \\times P_{\\text{bigram}} + \\lambda_3 \\times P_{\\text{unigram}} \\\\\n&= 0.7 \\times 0.8 + 0.2 \\times 0.5 + 0.1 \\times 0.3 \\\\\n&= 0.69\n\\end{align}\n$$\n\n**Why This Works:**\n- Trigrams: High precision, low coverage (main signal when available)\n- Bigrams: Medium precision, medium coverage (helpful fallback)\n- Unigrams: Low precision, full coverage (prevents zero probabilities)\n\n---\n\n### Connection to Previous Assignments\n\n| Concept | Assignment 2 (Neural Network) | Assignment 4 (N-grams) |\n|---------|-------------------------------|------------------------|\n| **Model Type** | Neural (learned weights) | Statistical (count-based) |\n| **Training** | Backpropagation + gradient descent | Count n-grams in corpus |\n| **Probability** | Sigmoid activation \u2192 [0,1] | Conditional probability P(w\\|context) |\n| **Smoothing** | Regularization, dropout | Laplace, add-k, interpolation |\n| **Unknown Handling** | Learn generalizations | Smoothing techniques |\n| **Evaluation** | Accuracy, confusion matrix | Perplexity |\n\n**Bridge to Neural LMs:** Later in the course, you'll see how neural networks can learn these probability distributions directly from data, combining the best of both approaches!\n\n---\n\nLet's implement each component!"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict, Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Q1: N-gram Probability Calculation\n\n### Goal\nCalculate the probability of a sentence using n-gram language models and the **chain rule of probability**.\n\n### Mathematical Foundation\n\n#### **Chain Rule of Probability**\nAny joint probability can be decomposed:\n\n$$\nP(w_1, w_2, w_3, \\ldots, w_n) = P(w_1) \\times P(w_2|w_1) \\times P(w_3|w_1,w_2) \\times \\cdots \\times P(w_n|w_1\\ldots w_{n-1})\n$$\n\n#### **Markov Assumption (N-gram approximation)**\nAssume each word depends only on the previous (n-1) words:\n\n**Bigram (n=2):**\n\n$$\nP(w_1, w_2, w_3) \\approx P(w_1) \\times P(w_2|w_1) \\times P(w_3|w_2)\n$$\n\n**Trigram (n=3):**\n\n$$\nP(w_1, w_2, w_3, w_4) \\approx P(w_1, w_2) \\times P(w_3|w_1,w_2) \\times P(w_4|w_2,w_3)\n$$\n\n#### **Maximum Likelihood Estimation (MLE)**\nEstimate probabilities from counts:\n\n$$\nP(w_n|w_1\\ldots w_{n-1}) = \\frac{C(w_1\\ldots w_n)}{C(w_1\\ldots w_{n-1})}\n$$\n\n### Example\n\n**Training Corpus:**\n```\n<s> I am happy </s>\n<s> I am learning </s>\n```\n\n**Bigram Counts:**\n\n$$\n\\begin{align}\nC(\\text{<s> I}) &= 2 \\\\\nC(\\text{I am}) &= 2 \\\\\nC(\\text{am happy}) &= 1 \\\\\nC(\\text{am learning}) &= 1\n\\end{align}\n$$\n\n**Calculate** $P(\\text{<s> I am happy </s>})$:\n\n$$\n\\begin{align}\nP(\\text{I}|\\text{<s>}) &= \\frac{C(\\text{<s> I})}{C(\\text{<s>})} = \\frac{2}{2} = 1.0 \\\\\nP(\\text{am}|\\text{I}) &= \\frac{C(\\text{I am})}{C(\\text{I})} = \\frac{2}{2} = 1.0 \\\\\nP(\\text{happy}|\\text{am}) &= \\frac{C(\\text{am happy})}{C(\\text{am})} = \\frac{1}{2} = 0.5 \\\\\nP(\\text{</s>}|\\text{happy}) &= \\frac{C(\\text{happy </s>})}{C(\\text{happy})} = \\frac{1}{1} = 1.0 \\\\\n\\\\\nP(\\text{sentence}) &= 1.0 \\times 1.0 \\times 0.5 \\times 1.0 = 0.5\n\\end{align}\n$$\n\n### Tips\n- Build count dictionaries for n-grams and (n-1)-grams\n- Use `defaultdict(int)` to handle missing keys gracefully\n- For trigrams, condition on previous 2 words: $P(w_3|w_1,w_2)$\n- Remember to include `<s>` and `</s>` tokens!"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample data for testing\n",
        "training_corpus = [\n",
        "    ['<s>', 'I', 'am', 'happy', '</s>'],\n",
        "    ['<s>', 'I', 'am', 'learning', '</s>'],\n",
        "    ['<s>', 'I', 'am', 'happy', 'because', 'I', 'am', 'learning', '</s>']\n",
        "]\n",
        "\n",
        "def build_ngram_counts(corpus, n):\n",
        "    \"\"\"\n",
        "    Build n-gram count dictionary from corpus.\n",
        "    \n",
        "    Args:\n",
        "        corpus: List of tokenized sentences\n",
        "        n: N-gram order (2 for bigram, 3 for trigram)\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary mapping n-grams (as tuples) to counts\n",
        "    \"\"\"\n",
        "    counts = defaultdict(int)\n",
        "    \n",
        "    for sentence in corpus:\n",
        "        # Slide window of size n over sentence\n",
        "        for i in range(len(sentence) - n + 1):\n",
        "            ngram = tuple(sentence[i:i+n])\n",
        "            counts[ngram] += 1\n",
        "    \n",
        "    return counts\n",
        "\n",
        "# Test the function\n",
        "bigram_counts = build_ngram_counts(training_corpus, 2)\n",
        "print(\"Bigram counts:\")\n",
        "for bigram, count in sorted(bigram_counts.items()):\n",
        "    print(f\"  {bigram}: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Q1: N-gram Probability\n",
        "\n",
        "def calculate_ngram_probability(sentence, ngram_counts, n):\n",
        "    \"\"\"\n",
        "    Calculate probability of a sentence using n-gram model.\n",
        "    \n",
        "    Args:\n",
        "        sentence: List of tokens (should include <s> and </s>)\n",
        "        ngram_counts: Dictionary of n-gram counts\n",
        "        n: Order of n-gram (2 for bigram, 3 for trigram)\n",
        "    \n",
        "    Returns:\n",
        "        Probability of the sentence\n",
        "    \"\"\"\n",
        "    # Build (n-1)-gram counts for denominator\n",
        "    prefix_counts = defaultdict(int)\n",
        "    for ngram in ngram_counts:\n",
        "        prefix = ngram[:-1]  # All but last word\n",
        "        prefix_counts[prefix] += ngram_counts[ngram]\n",
        "    \n",
        "    probability = 1.0\n",
        "    \n",
        "    # Calculate probability using chain rule\n",
        "    for i in range(len(sentence) - n + 1):\n",
        "        ngram = tuple(sentence[i:i+n])\n",
        "        prefix = ngram[:-1]\n",
        "        \n",
        "        # P(w\u2099|w\u2081...w\u2099\u208b\u2081) = C(w\u2081...w\u2099) / C(w\u2081...w\u2099\u208b\u2081)\n",
        "        if prefix_counts[prefix] > 0:\n",
        "            prob = ngram_counts[ngram] / prefix_counts[prefix]\n",
        "        else:\n",
        "            prob = 0  # Unseen context\n",
        "        \n",
        "        probability *= prob\n",
        "    \n",
        "    return probability\n",
        "\n",
        "# Test with through-line example\n",
        "test_sentence = ['<s>', 'I', 'am', 'happy', '</s>']\n",
        "prob = calculate_ngram_probability(test_sentence, bigram_counts, 2)\n",
        "print(f\"\\nP({' '.join(test_sentence)}) = {prob}\")\n",
        "print(f\"This means: The model assigns {prob*100:.1f}% probability to this sentence\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Q2: Laplace (Add-1) Smoothing\n\n### The Zero Probability Problem\n\n**Scenario:** Training corpus has never seen \"am sad\"\n\n$$\nP(\\text{sad}|\\text{am}) = \\frac{C(\\text{am sad})}{C(\\text{am})} = \\frac{0}{4} = 0\n$$\n\n**Consequence:** Entire sentence probability becomes 0!\n\n$$\n\\begin{align}\nP(\\text{I am sad}) &= P(\\text{I}) \\times P(\\text{am}|\\text{I}) \\times P(\\text{sad}|\\text{am}) \\\\\n&= 1.0 \\times 1.0 \\times 0 = 0 \\quad \u274c\n\\end{align}\n$$\n\n### Laplace Smoothing Solution\n\n**Idea:** Pretend we saw every possible n-gram at least once\n\n**Formula:**\n\n$$\nP_{\\text{laplace}}(w_n|w_1\\ldots w_{n-1}) = \\frac{C(w_1\\ldots w_n) + 1}{C(w_1\\ldots w_{n-1}) + V}\n$$\n\nwhere $V$ = vocabulary size (number of unique words)\n\n### Visual Intuition: Redistributing Probability Mass\n\n**Before Smoothing (Bigrams after \"am\"):**\n```\nam happy:   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 50%\nam learning:\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 50%\nam sad:                   0%  \u2190 Problem!\nam excited:               0%\n... (9,996 other words)   0%\n```\n\n**After Laplace Smoothing (V=10,000):**\n```\nam happy:   \u2588\u2588 0.03%  (was 50%)\nam learning:\u2588\u2588 0.03%  (was 50%)\nam sad:     \u2588  0.01%  (was 0%)  \u2713\nam excited: \u2588  0.01%  (was 0%)  \u2713\n... each of 10,000 words gets 0.01%\n```\n\n### Trade-off\n\u2705 **Benefit:** No more zero probabilities  \n\u274c **Cost:** Known n-grams lose most of their probability (50% \u2192 0.03%)\n\n### Example Calculation\n\n**Original:**\n\n$$\nC(\\text{am happy}) = 2, \\quad C(\\text{am}) = 4\n$$\n\n$$\nP(\\text{happy}|\\text{am}) = \\frac{2}{4} = 0.5\n$$\n\n**With Laplace (V = 10,000):**\n\n$$\nP_{\\text{laplace}}(\\text{happy}|\\text{am}) = \\frac{2 + 1}{4 + 10000} = \\frac{3}{10004} \\approx 0.0003\n$$\n\n**Unseen n-gram:**\n\n$$\nP_{\\text{laplace}}(\\text{sad}|\\text{am}) = \\frac{0 + 1}{4 + 10000} = \\frac{1}{10004} \\approx 0.0001\n$$\n\n### Tips\n- Always add V to denominator (not just V\u00d7n)\n- V should be vocabulary size, not number of n-grams\n- This method works for any n-gram order"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Q2: Laplace Smoothing\n",
        "\n",
        "def calculate_laplace_probability(ngram, ngram_counts, vocab_size):\n",
        "    \"\"\"\n",
        "    Calculate n-gram probability with Laplace (add-1) smoothing.\n",
        "    \n",
        "    Args:\n",
        "        ngram: Tuple of words (e.g., ('am', 'happy'))\n",
        "        ngram_counts: Dictionary of n-gram counts\n",
        "        vocab_size: Number of unique words in vocabulary\n",
        "    \n",
        "    Returns:\n",
        "        Smoothed probability\n",
        "    \"\"\"\n",
        "    # Get n-gram count (0 if unseen)\n",
        "    ngram_count = ngram_counts.get(ngram, 0)\n",
        "    \n",
        "    # Get prefix count (all but last word)\n",
        "    prefix = ngram[:-1]\n",
        "    prefix_count = sum(count for ng, count in ngram_counts.items() \n",
        "                      if ng[:-1] == prefix)\n",
        "    \n",
        "    # Laplace formula: (C + 1) / (C_prefix + V)\n",
        "    probability = (ngram_count + 1) / (prefix_count + vocab_size)\n",
        "    \n",
        "    return probability\n",
        "\n",
        "# Build vocabulary from training corpus\n",
        "vocabulary = set()\n",
        "for sentence in training_corpus:\n",
        "    vocabulary.update(sentence)\n",
        "V = len(vocabulary)\n",
        "\n",
        "print(f\"Vocabulary size: {V}\\n\")\n",
        "\n",
        "# Test with known bigram\n",
        "known_bigram = ('am', 'happy')\n",
        "prob_known = calculate_laplace_probability(known_bigram, bigram_counts, V)\n",
        "print(f\"P_laplace{known_bigram} = {prob_known:.4f}\")\n",
        "\n",
        "# Test with unseen bigram\n",
        "unseen_bigram = ('am', 'sad')\n",
        "prob_unseen = calculate_laplace_probability(unseen_bigram, bigram_counts, V)\n",
        "print(f\"P_laplace{unseen_bigram} = {prob_unseen:.4f}\")\n",
        "\n",
        "print(f\"\\nNotice: Unseen n-gram gets non-zero probability!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Q3: Perplexity\n\n### What is Perplexity?\n\n**Intuition:** Perplexity measures how \"confused\" or \"surprised\" your language model is by the test data.\n\n**Lower perplexity = Better model**\n\n### Mathematical Definition\n\nPerplexity is the **inverse probability** of the test set, normalized by the number of words:\n\n$$\n\\text{PP}(W) = P(w_1, w_2, \\ldots, w_n)^{-1/N}\n$$\n\nwhere $N$ is the number of words in test set.\n\n### Why Use Perplexity Instead of Probability?\n\n**Problem with raw probability:**\n\n$$\n\\begin{align}\nP(\\text{\"I am happy\"}) &= 0.5 \\quad \\text{(3 words)} \\\\\nP(\\text{\"I am happy because I am learning\"}) &= 0.01 \\quad \\text{(7 words)}\n\\end{align}\n$$\n\n*Longer sentences always have lower probability! Can't compare.*\n\n**Solution with perplexity:**\n\n$$\n\\begin{align}\n\\text{PP}(\\text{\"I am happy\"}) &= 0.5^{-1/3} = 1.26 \\\\\n\\text{PP}(\\text{\"I am happy because I am learning\"}) &= 0.01^{-1/7} = 1.86\n\\end{align}\n$$\n\n*Now we can compare! First sentence has lower perplexity \u2192 better modeled*\n\n### Interpretation Guide\n\n| Perplexity | Meaning | Example |\n|------------|---------|----------|\n| **1** | Perfect prediction | Model assigns P=1 to every word |\n| **10** | Choosing from ~10 words | Model is uncertain among 10 likely words |\n| **100** | Choosing from ~100 words | Model is confused, like random guessing from 100 options |\n| **1000+** | Very confused | Poor model, almost random |\n\n### Step-by-Step Example\n\n**Test sentence:** \"I am happy\"  \n**Bigram probabilities:** $P(\\text{I}|\\text{<s>})=1.0$, $P(\\text{am}|\\text{I})=1.0$, $P(\\text{happy}|\\text{am})=0.5$, $P(\\text{</s>}|\\text{happy})=1.0$\n\n**Step 1: Calculate sentence probability**\n\n$$\nP(\\text{sentence}) = 1.0 \\times 1.0 \\times 0.5 \\times 1.0 = 0.5\n$$\n\n**Step 2: Count words (excluding `<s>`, `</s>`)**\n\n$$\nN = 3 \\quad \\text{[I, am, happy]}\n$$\n\n**Step 3: Calculate perplexity**\n\n$$\n\\text{PP} = 0.5^{-1/3} = \\frac{1}{0.5^{1/3}} = \\frac{1}{0.794} = 1.26\n$$\n\n**Interpretation:** The model is about as confused as if it were choosing from 1.26 equally likely words. This is very good!\n\n### Connection to Cross-Entropy\n\nPerplexity is $2^{\\text{cross-entropy}}$:\n\n$$\n\\begin{align}\n\\text{Cross-Entropy} &= -\\frac{1}{N} \\sum \\log_2 P(w_i|\\text{context}) \\\\\n\\text{Perplexity} &= 2^{\\text{Cross-Entropy}}\n\\end{align}\n$$\n\nThis connects to information theory: perplexity measures average branching factor.\n\n### Tips\n- Use `math.pow()` for fractional exponents\n- Typically count only content words (exclude `<s>` and `</s>`)\n- Lower perplexity = better language model\n- Compare perplexity only on the same test set"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Q3: Perplexity\n",
        "\n",
        "def calculate_perplexity(test_sentences, ngram_counts, n):\n",
        "    \"\"\"\n",
        "    Calculate perplexity of test set given n-gram model.\n",
        "    \n",
        "    Args:\n",
        "        test_sentences: List of tokenized test sentences\n",
        "        ngram_counts: Dictionary of n-gram counts from training\n",
        "        n: Order of n-gram\n",
        "    \n",
        "    Returns:\n",
        "        Perplexity value\n",
        "    \"\"\"\n",
        "    total_log_prob = 0\n",
        "    total_words = 0\n",
        "    \n",
        "    # Build prefix counts\n",
        "    prefix_counts = defaultdict(int)\n",
        "    for ngram in ngram_counts:\n",
        "        prefix = ngram[:-1]\n",
        "        prefix_counts[prefix] += ngram_counts[ngram]\n",
        "    \n",
        "    for sentence in test_sentences:\n",
        "        # Count words (excluding <s> and </s>)\n",
        "        words = [w for w in sentence if w not in ['<s>', '</s>']]\n",
        "        total_words += len(words)\n",
        "        \n",
        "        # Calculate log probability of sentence\n",
        "        for i in range(len(sentence) - n + 1):\n",
        "            ngram = tuple(sentence[i:i+n])\n",
        "            prefix = ngram[:-1]\n",
        "            \n",
        "            if prefix_counts[prefix] > 0:\n",
        "                prob = ngram_counts[ngram] / prefix_counts[prefix]\n",
        "            else:\n",
        "                prob = 1e-10  # Small probability for unseen\n",
        "            \n",
        "            # Avoid log(0)\n",
        "            if prob > 0:\n",
        "                total_log_prob += math.log(prob)\n",
        "            else:\n",
        "                total_log_prob += math.log(1e-10)\n",
        "    \n",
        "    # Perplexity = P(test)^(-1/N)\n",
        "    avg_log_prob = total_log_prob / total_words\n",
        "    perplexity = math.exp(-avg_log_prob)\n",
        "    \n",
        "    return perplexity\n",
        "\n",
        "# Test on training data (should be low perplexity)\n",
        "test_set = [['<s>', 'I', 'am', 'happy', '</s>']]\n",
        "pp = calculate_perplexity(test_set, bigram_counts, 2)\n",
        "print(f\"Perplexity on 'I am happy': {pp:.2f}\")\n",
        "print(f\"Interpretation: Model is as confused as choosing from {pp:.1f} equally likely words\")\n",
        "\n",
        "# Test on unseen data\n",
        "unseen_test = [['<s>', 'I', 'am', 'sad', '</s>']]\n",
        "pp_unseen = calculate_perplexity(unseen_test, bigram_counts, 2)\n",
        "print(f\"\\nPerplexity on 'I am sad': {pp_unseen:.2f}\")\n",
        "print(f\"Higher perplexity indicates unseen n-gram!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Q4: Add-k Smoothing\n\n### The Laplace Problem\n\nLaplace (add-1) smoothing is too aggressive:\n\n$$\n\\begin{align}\n\\text{With } V &= 10,000: \\\\\nP_{\\text{laplace}}(\\text{happy}|\\text{am}) &= \\frac{2 + 1}{4 + 10000} = 0.0003 \\\\\nP_{\\text{laplace}}(\\text{sad}|\\text{am}) &= \\frac{0 + 1}{4 + 10000} = 0.0001\n\\end{align}\n$$\n\n*Ratio is only 3:1, but original was infinite!  \nUnseen n-grams get too much probability*\n\n### Add-k Smoothing Solution\n\n**Idea:** Add a smaller value $k$ instead of always adding 1\n\n**Formula:**\n\n$$\nP_k(w_n|w_1\\ldots w_{n-1}) = \\frac{C(w_1\\ldots w_n) + k}{C(w_1\\ldots w_{n-1}) + k \\times V}\n$$\n\n### Comparing Different k Values\n\nGiven: $C(\\text{am happy}) = 2$, $C(\\text{am}) = 4$, $V = 10,000$\n\n| k value | $P(\\text{happy}|\\text{am})$ | $P(\\text{sad}|\\text{am})$ | Ratio | Comment |\n|---------|--------------|------------|-------|----------|\n| **k = 0** | $2/4 = 0.5$ | $0/4 = 0$ | \u221e | No smoothing, zero prob problem |\n| **k = 0.01** | $2.01/4.1 = 0.490$ | $0.01/4.1 = 0.0024$ | 200:1 | Gentle smoothing |\n| **k = 0.1** | $2.1/14 = 0.150$ | $0.1/14 = 0.0071$ | 21:1 | Light smoothing |\n| **k = 1** | $3/10004 = 0.0003$ | $1/10004 = 0.0001$ | 3:1 | Laplace (too aggressive) |\n\n### Choosing k\n\n**Rule of thumb:**\n- $k \\in [0.01, 0.1]$ for most tasks\n- Smaller $k$ = preserve original probabilities more\n- Larger $k$ = redistribute more probability to unseen\n\n**Selection method:**\n1. Try different $k$ values on validation set\n2. Choose $k$ that gives lowest perplexity\n3. Different $k$ works better for different corpus sizes\n\n### Visual Comparison\n\n**k = 0 (No smoothing):**\n```\nam happy:   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 50%\nam learning:\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 50%\nam sad:                               0%  \u274c\n```\n\n**k = 0.1 (Add-k):**\n```\nam happy:   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 15%\nam learning:\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 15%\nam sad:     \u2588 0.7%  \u2713 (small but non-zero)\n```\n\n**k = 1 (Laplace):**\n```\nam happy:   \u2588 0.03%\nam learning:\u2588 0.03%\nam sad:     \u2588 0.01%\n# All probabilities too small!\n```\n\n### Tips\n- Add $k$ to numerator, $k \\times V$ to denominator (not just $V$)\n- When $k=1$, this becomes Laplace smoothing\n- Smaller $k$ usually works better for large vocabularies\n- Use validation set to tune $k$"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Q4: Add-k Smoothing\n",
        "\n",
        "def calculate_add_k_probability(ngram, ngram_counts, vocab_size, k):\n",
        "    \"\"\"\n",
        "    Calculate n-gram probability with add-k smoothing.\n",
        "    \n",
        "    Args:\n",
        "        ngram: Tuple of words\n",
        "        ngram_counts: Dictionary of n-gram counts\n",
        "        vocab_size: Number of unique words\n",
        "        k: Smoothing parameter (0 < k \u2264 1)\n",
        "    \n",
        "    Returns:\n",
        "        Smoothed probability\n",
        "    \"\"\"\n",
        "    # Get n-gram count\n",
        "    ngram_count = ngram_counts.get(ngram, 0)\n",
        "    \n",
        "    # Get prefix count\n",
        "    prefix = ngram[:-1]\n",
        "    prefix_count = sum(count for ng, count in ngram_counts.items() \n",
        "                      if ng[:-1] == prefix)\n",
        "    \n",
        "    # Add-k formula: (C + k) / (C_prefix + k\u00d7V)\n",
        "    probability = (ngram_count + k) / (prefix_count + k * vocab_size)\n",
        "    \n",
        "    return probability\n",
        "\n",
        "# Compare different k values\n",
        "k_values = [0, 0.01, 0.1, 0.5, 1.0]\n",
        "test_bigrams = [('am', 'happy'), ('am', 'sad')]  # Known and unknown\n",
        "\n",
        "print(\"Comparing different k values:\\n\")\n",
        "print(f\"{'k':<6} | {'P(happy|am)':<12} | {'P(sad|am)':<12} | Ratio\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for k in k_values:\n",
        "    if k == 0:\n",
        "        # No smoothing - manual calculation\n",
        "        p_happy = 2/4  # C(am happy) / C(am)\n",
        "        p_sad = 0\n",
        "        ratio = \"\u221e\" if p_sad == 0 else f\"{p_happy/p_sad:.1f}:1\"\n",
        "    else:\n",
        "        p_happy = calculate_add_k_probability(test_bigrams[0], bigram_counts, V, k)\n",
        "        p_sad = calculate_add_k_probability(test_bigrams[1], bigram_counts, V, k)\n",
        "        ratio = f\"{p_happy/p_sad:.1f}:1\" if p_sad > 0 else \"\u221e\"\n",
        "    \n",
        "    print(f\"{k:<6} | {p_happy:<12.6f} | {p_sad:<12.6f} | {ratio}\")\n",
        "\n",
        "print(\"\\nObservation: Smaller k preserves the probability ratio better!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Q5: Linear Interpolation\n\n### The Smoothing Hierarchy Problem\n\nDifferent n-gram orders have different strengths:\n\n| N-gram | Strength | Weakness | Example |\n|--------|----------|----------|----------|\n| **Trigram** | High precision (specific context) | Low coverage (sparse) | $P(\\text{happy}|\\text{I am}) = 0.8$ (but many unseen trigrams) |\n| **Bigram** | Medium precision | Medium coverage | $P(\\text{happy}|\\text{am}) = 0.5$ (more data, less specific) |\n| **Unigram** | Always available | No context | $P(\\text{happy}) = 0.3$ (just word frequency) |\n\n### Interpolation Solution\n\n**Idea:** Combine all n-gram orders with learned weights!\n\n**Formula:**\n\n$$\n\\begin{align}\nP_{\\text{interp}}(w_3|w_1,w_2) &= \\lambda_1 \\times P(w_3|w_1,w_2) \\quad \\text{# Trigram} \\\\\n&+ \\lambda_2 \\times P(w_3|w_2) \\quad \\text{# Bigram} \\\\\n&+ \\lambda_3 \\times P(w_3) \\quad \\text{# Unigram}\n\\end{align}\n$$\n\nwhere $\\lambda_1 + \\lambda_2 + \\lambda_3 = 1$ (weights sum to 1)\n\n### Why This Works\n\n**Case 1: Seen trigram**\n\n$$\n\\begin{align}\n\\text{\"I am happy\"} &\\text{ appears often in training} \\\\\nP_{\\text{trigram}}(\\text{happy}|\\text{I,am}) &= 0.8 \\quad \\text{(High confidence)} \\\\\nP_{\\text{bigram}}(\\text{happy}|\\text{am}) &= 0.5 \\quad \\text{(Medium)} \\\\\nP_{\\text{unigram}}(\\text{happy}) &= 0.3 \\quad \\text{(Low)}\n\\end{align}\n$$\n\n$$\n\\begin{align}\n\\text{Typical weights: } &\\lambda_1=0.7, \\lambda_2=0.2, \\lambda_3=0.1 \\\\\nP_{\\text{interp}} &= 0.7 \\times 0.8 + 0.2 \\times 0.5 + 0.1 \\times 0.3 = 0.69\n\\end{align}\n$$\n\n*Trigram dominates \u2713*\n\n**Case 2: Unseen trigram, seen bigram**\n\n$$\n\\begin{align}\n\\text{\"I am excited\"} &\\text{ never seen, but \"am excited\" exists} \\\\\nP_{\\text{trigram}}(\\text{excited}|\\text{I,am}) &= 0.0 \\quad \\text{(No data)} \\\\\nP_{\\text{bigram}}(\\text{excited}|\\text{am}) &= 0.4 \\quad \\text{(Has data!)} \\\\\nP_{\\text{unigram}}(\\text{excited}) &= 0.2 \\quad \\text{(Background)}\n\\end{align}\n$$\n\n$$\nP_{\\text{interp}} = 0.7 \\times 0.0 + 0.2 \\times 0.4 + 0.1 \\times 0.2 = 0.10\n$$\n\n*Bigram saves us! \u2713*\n\n**Case 3: Completely unseen words**\n\n$$\n\\begin{align}\n\\text{\"I am discombobulated\"} &\\text{ - never seen anything} \\\\\nP_{\\text{trigram}}(\\text{discombobulated}|\\text{I,am}) &= 0.0 \\\\\nP_{\\text{bigram}}(\\text{discombobulated}|\\text{am}) &= 0.0 \\\\\nP_{\\text{unigram}}(\\text{discombobulated}) &= 0.0001 \\quad \\text{(Rare word)}\n\\end{align}\n$$\n\n$$\nP_{\\text{interp}} = 0.7 \\times 0.0 + 0.2 \\times 0.0 + 0.1 \\times 0.0001 = 0.00001\n$$\n\n*Unigram provides fallback \u2713*\n\n### Learning Lambda Weights\n\n**Method: Optimize on held-out validation set**\n\n```python\n# 1. Split data: 80% train, 10% validation, 10% test\n# 2. Build n-gram models on training set\n# 3. Try different \u03bb combinations on validation set:\n\nfor \u03bb\u2081 in [0.5, 0.6, 0.7, 0.8]:\n    for \u03bb\u2082 in [0.1, 0.2, 0.3]:\n        \u03bb\u2083 = 1 - \u03bb\u2081 - \u03bb\u2082  # Ensure sum = 1\n        perplexity = evaluate_on_validation(\u03bb\u2081, \u03bb\u2082, \u03bb\u2083)\n        \n# 4. Choose \u03bb values with lowest perplexity\n# Typical result: \u03bb\u2081 \u2248 0.7, \u03bb\u2082 \u2248 0.2, \u03bb\u2083 \u2248 0.1\n```\n\n### Comparison with Other Methods\n\n| Method | Unseen Trigram | Seen Bigram | Seen Unigram | Result |\n|--------|----------------|-------------|--------------|--------|\n| **No smoothing** | 0 | 0.4 | 0.2 | **0** (fails!) |\n| **Laplace k=1** | 0.0001 | 0.0004 | 0.002 | 0.0001 (all too small) |\n| **Add-k (k=0.1)** | 0.001 | 0.08 | 0.15 | 0.001 (better) |\n| **Back-off** | Uses bigram | 0.4 | 0.2 | 0.4 (uses bigram only) |\n| **Interpolation** | Blends all | 0.4 | 0.2 | **0.10** (combines strengths!) |\n\n### Tips\n- Weights must sum to 1: $\\lambda_1 + \\lambda_2 + \\lambda_3 = 1$\n- Higher-order n-grams usually get larger weights ($\\lambda_1 > \\lambda_2 > \\lambda_3$)\n- Can combine with smoothing: interpolate smoothed probabilities\n- Use EM algorithm for more sophisticated weight learning"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Q5: Linear Interpolation\n",
        "\n",
        "def calculate_interpolated_probability(trigram, unigram_counts, bigram_counts, \n",
        "                                       trigram_counts, lambda_1, lambda_2, lambda_3):\n",
        "    \"\"\"\n",
        "    Calculate probability using linear interpolation of trigram, bigram, unigram.\n",
        "    \n",
        "    Args:\n",
        "        trigram: Tuple of 3 words (w1, w2, w3)\n",
        "        unigram_counts: Dictionary of word counts\n",
        "        bigram_counts: Dictionary of bigram counts\n",
        "        trigram_counts: Dictionary of trigram counts\n",
        "        lambda_1, lambda_2, lambda_3: Weights (must sum to 1)\n",
        "    \n",
        "    Returns:\n",
        "        Interpolated probability\n",
        "    \"\"\"\n",
        "    w1, w2, w3 = trigram\n",
        "    \n",
        "    # Calculate trigram probability P(w3|w1,w2)\n",
        "    trigram_prefix = (w1, w2)\n",
        "    trigram_prefix_count = sum(c for ng, c in trigram_counts.items() \n",
        "                               if ng[:2] == trigram_prefix)\n",
        "    if trigram_prefix_count > 0:\n",
        "        p_trigram = trigram_counts.get(trigram, 0) / trigram_prefix_count\n",
        "    else:\n",
        "        p_trigram = 0\n",
        "    \n",
        "    # Calculate bigram probability P(w3|w2)\n",
        "    bigram = (w2, w3)\n",
        "    bigram_prefix = (w2,)\n",
        "    bigram_prefix_count = sum(c for ng, c in bigram_counts.items() \n",
        "                             if ng[:1] == bigram_prefix)\n",
        "    if bigram_prefix_count > 0:\n",
        "        p_bigram = bigram_counts.get(bigram, 0) / bigram_prefix_count\n",
        "    else:\n",
        "        p_bigram = 0\n",
        "    \n",
        "    # Calculate unigram probability P(w3)\n",
        "    total_words = sum(unigram_counts.values())\n",
        "    p_unigram = unigram_counts.get(w3, 0) / total_words\n",
        "    \n",
        "    # Linear interpolation\n",
        "    p_interpolated = (lambda_1 * p_trigram + \n",
        "                     lambda_2 * p_bigram + \n",
        "                     lambda_3 * p_unigram)\n",
        "    \n",
        "    return p_interpolated, p_trigram, p_bigram, p_unigram\n",
        "\n",
        "# Build unigram and trigram counts from our corpus\n",
        "unigram_counts = build_ngram_counts(training_corpus, 1)\n",
        "# Convert tuples to single words for unigrams\n",
        "unigram_counts = {k[0]: v for k, v in unigram_counts.items()}\n",
        "\n",
        "trigram_counts = build_ngram_counts(training_corpus, 3)\n",
        "\n",
        "# Test with typical weights\n",
        "lambda_1, lambda_2, lambda_3 = 0.7, 0.2, 0.1\n",
        "\n",
        "print(f\"Lambda weights: \u03bb\u2081={lambda_1}, \u03bb\u2082={lambda_2}, \u03bb\u2083={lambda_3}\")\n",
        "print(f\"(Weights sum to {lambda_1 + lambda_2 + lambda_3})\\n\")\n",
        "\n",
        "# Test Case 1: Seen trigram\n",
        "test_trigram = ('I', 'am', 'happy')\n",
        "p_int, p_tri, p_bi, p_uni = calculate_interpolated_probability(\n",
        "    test_trigram, unigram_counts, bigram_counts, trigram_counts,\n",
        "    lambda_1, lambda_2, lambda_3\n",
        ")\n",
        "\n",
        "print(f\"Test: {test_trigram}\")\n",
        "print(f\"  P_trigram(happy|I,am) = {p_tri:.3f}\")\n",
        "print(f\"  P_bigram(happy|am)    = {p_bi:.3f}\")\n",
        "print(f\"  P_unigram(happy)      = {p_uni:.3f}\")\n",
        "print(f\"  P_interpolated        = {p_int:.3f}\")\n",
        "print(f\"  Calculation: {lambda_1}\u00d7{p_tri:.3f} + {lambda_2}\u00d7{p_bi:.3f} + {lambda_3}\u00d7{p_uni:.3f} = {p_int:.3f}\")\n",
        "\n",
        "# Test Case 2: Unseen trigram (but seen bigram)\n",
        "print(f\"\\n\" + \"=\"*50)\n",
        "test_trigram2 = ('am', 'learning', '</s>')\n",
        "p_int2, p_tri2, p_bi2, p_uni2 = calculate_interpolated_probability(\n",
        "    test_trigram2, unigram_counts, bigram_counts, trigram_counts,\n",
        "    lambda_1, lambda_2, lambda_3\n",
        ")\n",
        "\n",
        "print(f\"Test: {test_trigram2}\")\n",
        "print(f\"  P_trigram(</s>|am,learning) = {p_tri2:.3f}\")\n",
        "print(f\"  P_bigram(</s>|learning)     = {p_bi2:.3f}\")\n",
        "print(f\"  P_unigram(</s>)             = {p_uni2:.3f}\")\n",
        "print(f\"  P_interpolated              = {p_int2:.3f}\")\n",
        "print(f\"  Note: Bigram and unigram compensate for unseen trigram!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Putting It All Together: Complete Language Model Pipeline\n",
        "\n",
        "Now let's demonstrate the complete pipeline with all techniques:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"COMPLETE N-GRAM LANGUAGE MODEL PIPELINE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\n1. TRAINING CORPUS:\")\n",
        "for i, sent in enumerate(training_corpus, 1):\n",
        "    print(f\"   {i}. {' '.join(sent)}\")\n",
        "\n",
        "print(\"\\n2. N-GRAM COUNTS:\")\n",
        "print(f\"   Vocabulary size: {V}\")\n",
        "print(f\"   Unique bigrams: {len(bigram_counts)}\")\n",
        "print(f\"   Unique trigrams: {len(trigram_counts)}\")\n",
        "\n",
        "print(\"\\n3. PROBABILITY METHODS COMPARISON:\")\n",
        "test_sent = ['<s>', 'I', 'am', 'happy', '</s>']\n",
        "print(f\"   Test sentence: {' '.join(test_sent)}\\n\")\n",
        "\n",
        "# Method 1: Plain MLE\n",
        "prob_mle = calculate_ngram_probability(test_sent, bigram_counts, 2)\n",
        "print(f\"   Plain MLE:              P = {prob_mle:.4f}\")\n",
        "\n",
        "# Method 2: Laplace smoothing  \n",
        "prob_laplace = 1.0\n",
        "for i in range(len(test_sent) - 1):\n",
        "    bg = tuple(test_sent[i:i+2])\n",
        "    prob_laplace *= calculate_laplace_probability(bg, bigram_counts, V)\n",
        "print(f\"   Laplace (k=1):          P = {prob_laplace:.4f}\")\n",
        "\n",
        "# Method 3: Add-k smoothing\n",
        "prob_add_k = 1.0\n",
        "k = 0.1\n",
        "for i in range(len(test_sent) - 1):\n",
        "    bg = tuple(test_sent[i:i+2])\n",
        "    prob_add_k *= calculate_add_k_probability(bg, bigram_counts, V, k)\n",
        "print(f\"   Add-k (k=0.1):          P = {prob_add_k:.4f}\")\n",
        "\n",
        "print(\"\\n4. PERPLEXITY EVALUATION:\")\n",
        "pp_mle = calculate_perplexity([test_sent], bigram_counts, 2)\n",
        "print(f\"   Plain MLE perplexity:   {pp_mle:.2f}\")\n",
        "print(f\"   Interpretation: Model choosing from ~{pp_mle:.0f} equally likely words\")\n",
        "\n",
        "print(\"\\n5. INTERPOLATION (for trigrams):\")\n",
        "# Build sentence with trigram context\n",
        "trigram_example = ('I', 'am', 'happy')\n",
        "p_int, p_tri, p_bi, p_uni = calculate_interpolated_probability(\n",
        "    trigram_example, unigram_counts, bigram_counts, trigram_counts,\n",
        "    0.7, 0.2, 0.1\n",
        ")\n",
        "print(f\"   P_interp(happy|I,am) = {p_int:.4f}\")\n",
        "print(f\"   (Combined: {0.7}\u00d7{p_tri:.2f} + {0.2}\u00d7{p_bi:.2f} + {0.1}\u00d7{p_uni:.2f})\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"SUMMARY: Choose method based on your needs:\")\n",
        "print(\"  - Plain MLE: Fast, but fails on unseen n-grams\")\n",
        "print(\"  - Laplace: Simple, but too aggressive\")\n",
        "print(\"  - Add-k: Better balance, tune k on validation set\")\n",
        "print(\"  - Interpolation: Best performance, combines all orders\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Next Steps: From N-grams to Neural Language Models\n",
        "\n",
        "### Limitations of N-gram Models\n",
        "\n",
        "1. **Sparsity:** Most possible n-grams never appear in training\n",
        "2. **Fixed context:** Can't capture dependencies beyond n-1 words\n",
        "3. **No generalization:** \"cat sat\" and \"dog sat\" treated as completely different\n",
        "4. **Storage:** Need to store counts for millions of n-grams\n",
        "\n",
        "### Neural Language Models (Coming Soon!)\n",
        "\n",
        "**Key improvements:**\n",
        "- **Learned representations:** Words with similar meanings get similar vectors\n",
        "- **Generalization:** If model knows \"cat sat\", it can generalize to \"dog sat\"\n",
        "- **Variable context:** RNNs/Transformers can use arbitrarily long context\n",
        "- **Compact:** Store learned weights instead of all n-gram counts\n",
        "\n",
        "**But n-grams are still valuable:**\n",
        "- Baseline for comparison\n",
        "- Interpretable (can inspect exact counts)\n",
        "- No training required (just count!)\n",
        "- Still used in hybrid models\n",
        "\n",
        "You're now ready to understand how modern language models like GPT build on these foundations!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}