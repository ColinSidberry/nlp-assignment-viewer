{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: N-gram Language Models\n",
    "\n",
    "## ML Pipeline Overview\n",
    "\n",
    "This assignment demonstrates a complete ML pipeline for **n-gram language modeling with smoothing and interpolation**:\n",
    "\n",
    "### ML Goal\n",
    "Build a probabilistic language model that can:\n",
    "1. **Estimate probabilities** of word sequences using n-grams\n",
    "2. **Handle unseen n-grams** using smoothing techniques\n",
    "3. **Evaluate model quality** using perplexity\n",
    "4. **Combine multiple models** using interpolation\n",
    "\n",
    "### Training Data\n",
    "Corpus of sentences used to compute n-gram counts and probabilities\n",
    "\n",
    "---\n",
    "\n",
    "### Through-Line Example: \"I am happy\"\n",
    "\n",
    "Let's trace how we build a language model for the sentence **\"I am happy\"**:\n",
    "\n",
    "#### **Progression from Assignment 1 to Assignment 4**\n",
    "\n",
    "| Assignment | Task | Method | Example |\n",
    "|------------|------|--------|----------|\n",
    "| **A1: Autocomplete** | Complete partial word | Frequency-based prefix matching | \"happ\" → \"happy\" (most frequent match) |\n",
    "| **A3: Spell-check** | Correct misspelled word | Edit distance + frequency | \"happpy\" → \"happy\" (1 edit, high frequency) |\n",
    "| **A4: Language Model** | Predict next word probability | N-gram conditional probability | \"I am\" → P(happy\\|I am) = 0.6 |\n",
    "\n",
    "**Key Evolution:** \n",
    "- A1: \"What's the most frequent word with this prefix?\"\n",
    "- A3: \"What's the most likely correction given edit distance?\"\n",
    "- A4: **\"What's the probability of this word given the previous words?\"**\n",
    "\n",
    "---\n",
    "\n",
    "### Pipeline Stages\n",
    "\n",
    "#### **1. N-gram Probability Calculation (Q1)**\n",
    "\n",
    "**Training Corpus:**\n",
    "```\n",
    "I am happy\n",
    "I am learning\n",
    "I am happy because I am learning\n",
    "```\n",
    "\n",
    "**Build N-gram Counts:**\n",
    "```python\n",
    "# Bigram counts\n",
    "C(I am) = 3\n",
    "C(am happy) = 2\n",
    "C(am learning) = 2\n",
    "\n",
    "# Calculate probability using MLE (Maximum Likelihood Estimation)\n",
    "P(happy|am) = C(am happy) / C(am) = 2 / 4 = 0.5\n",
    "P(learning|am) = C(am learning) / C(am) = 2 / 4 = 0.5\n",
    "```\n",
    "\n",
    "**Apply Chain Rule for Full Sentence:**\n",
    "```python\n",
    "P(I am happy) = P(I) × P(am|I) × P(happy|am)\n",
    "              = 1.0 × 1.0 × 0.5 = 0.5\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Laplace Smoothing (Q2)**\n",
    "\n",
    "**Problem:** What if we see \"am sad\" in test set, but it never appeared in training?\n",
    "```python\n",
    "P(sad|am) = C(am sad) / C(am) = 0 / 4 = 0  ❌ Zero probability!\n",
    "```\n",
    "\n",
    "**Solution: Add-1 (Laplace) Smoothing**\n",
    "```python\n",
    "# Add 1 to all counts\n",
    "P_laplace(sad|am) = (C(am sad) + 1) / (C(am) + V)\n",
    "                  = (0 + 1) / (4 + 10000)  # V = vocabulary size\n",
    "                  = 0.0001  ✓ Small but non-zero!\n",
    "```\n",
    "\n",
    "**Effect on Known N-grams:**\n",
    "```python\n",
    "P_laplace(happy|am) = (2 + 1) / (4 + 10000) = 0.0003\n",
    "# Note: All probabilities reduced, but unknown gets non-zero value\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Perplexity Evaluation (Q3)**\n",
    "\n",
    "**Goal:** Measure how \"surprised\" the model is by the test set\n",
    "\n",
    "```python\n",
    "# Test sentence: \"I am happy\"\n",
    "# Calculate probability\n",
    "P(I am happy) = 0.5\n",
    "\n",
    "# Perplexity = inverse geometric mean of probabilities\n",
    "PP = P(test_set)^(-1/M) where M = number of words\n",
    "PP = 0.5^(-1/3) = 1.26\n",
    "```\n",
    "\n",
    "**Interpretation:**\n",
    "- **Lower perplexity = better model** (less surprised)\n",
    "- PP = 1: Perfect prediction (model assigns probability 1)\n",
    "- PP = 100: Model is as confused as if choosing randomly from 100 words\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Add-k Smoothing (Q4)**\n",
    "\n",
    "**Problem:** Laplace (k=1) gives too much probability to unseen n-grams\n",
    "\n",
    "**Solution: Use smaller k (e.g., k=0.1)**\n",
    "```python\n",
    "P_k(sad|am) = (0 + 0.1) / (4 + 0.1 × 10000) = 0.00001\n",
    "P_k(happy|am) = (2 + 0.1) / (4 + 0.1 × 10000) = 0.00021\n",
    "\n",
    "# Benefit: Known n-grams keep more of their probability mass\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. Linear Interpolation (Q5)**\n",
    "\n",
    "**Idea:** Combine trigram, bigram, and unigram probabilities with weights\n",
    "\n",
    "```python\n",
    "# Given: \"I am happy\"\n",
    "P_trigram(happy|I am) = 0.8\n",
    "P_bigram(happy|am) = 0.5\n",
    "P_unigram(happy) = 0.3\n",
    "\n",
    "# Weighted combination (λ₁ + λ₂ + λ₃ = 1)\n",
    "P_interpolated = λ₁ × P_trigram + λ₂ × P_bigram + λ₃ × P_unigram\n",
    "                = 0.7 × 0.8 + 0.2 × 0.5 + 0.1 × 0.3\n",
    "                = 0.69\n",
    "```\n",
    "\n",
    "**Why This Works:**\n",
    "- Trigrams: High precision, low coverage (main signal when available)\n",
    "- Bigrams: Medium precision, medium coverage (helpful fallback)\n",
    "- Unigrams: Low precision, full coverage (prevents zero probabilities)\n",
    "\n",
    "---\n",
    "\n",
    "### Connection to Previous Assignments\n",
    "\n",
    "| Concept | Assignment 2 (Neural Network) | Assignment 4 (N-grams) |\n",
    "|---------|-------------------------------|------------------------|\n",
    "| **Model Type** | Neural (learned weights) | Statistical (count-based) |\n",
    "| **Training** | Backpropagation + gradient descent | Count n-grams in corpus |\n",
    "| **Probability** | Sigmoid activation → [0,1] | Conditional probability P(w\\|context) |\n",
    "| **Smoothing** | Regularization, dropout | Laplace, add-k, interpolation |\n",
    "| **Unknown Handling** | Learn generalizations | Smoothing techniques |\n",
    "| **Evaluation** | Accuracy, confusion matrix | Perplexity |\n",
    "\n",
    "**Bridge to Neural LMs:** Later in the course, you'll see how neural networks can learn these probability distributions directly from data, combining the best of both approaches!\n",
    "\n",
    "---\n",
    "\n",
    "Let's implement each component!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q1: N-gram Probability Calculation\n",
    "\n",
    "### Goal\n",
    "Calculate the probability of a sentence using n-gram language models and the **chain rule of probability**.\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "#### **Chain Rule of Probability**\n",
    "Any joint probability can be decomposed:\n",
    "```\n",
    "P(w₁, w₂, w₃, ..., wₙ) = P(w₁) × P(w₂|w₁) × P(w₃|w₁,w₂) × ... × P(wₙ|w₁...wₙ₋₁)\n",
    "```\n",
    "\n",
    "#### **Markov Assumption (N-gram approximation)**\n",
    "Assume each word depends only on the previous (n-1) words:\n",
    "\n",
    "**Bigram (n=2):**\n",
    "```\n",
    "P(w₁, w₂, w₃) ≈ P(w₁) × P(w₂|w₁) × P(w₃|w₂)\n",
    "```\n",
    "\n",
    "**Trigram (n=3):**\n",
    "```\n",
    "P(w₁, w₂, w₃, w₄) ≈ P(w₁, w₂) × P(w₃|w₁,w₂) × P(w₄|w₂,w₃)\n",
    "```\n",
    "\n",
    "#### **Maximum Likelihood Estimation (MLE)**\n",
    "Estimate probabilities from counts:\n",
    "```\n",
    "P(wₙ|w₁...wₙ₋₁) = C(w₁...wₙ) / C(w₁...wₙ₋₁)\n",
    "```\n",
    "\n",
    "### Example\n",
    "\n",
    "**Training Corpus:**\n",
    "```\n",
    "<s> I am happy </s>\n",
    "<s> I am learning </s>\n",
    "```\n",
    "\n",
    "**Bigram Counts:**\n",
    "```python\n",
    "C(<s> I) = 2\n",
    "C(I am) = 2\n",
    "C(am happy) = 1\n",
    "C(am learning) = 1\n",
    "```\n",
    "\n",
    "**Calculate P(<s> I am happy </s>):**\n",
    "```python\n",
    "P(I|<s>) = C(<s> I) / C(<s>) = 2/2 = 1.0\n",
    "P(am|I) = C(I am) / C(I) = 2/2 = 1.0\n",
    "P(happy|am) = C(am happy) / C(am) = 1/2 = 0.5\n",
    "P(</s>|happy) = C(happy </s>) / C(happy) = 1/1 = 1.0\n",
    "\n",
    "P(sentence) = 1.0 × 1.0 × 0.5 × 1.0 = 0.5\n",
    "```\n",
    "\n",
    "### Tips\n",
    "- Build count dictionaries for n-grams and (n-1)-grams\n",
    "- Use `defaultdict(int)` to handle missing keys gracefully\n",
    "- For trigrams, condition on previous 2 words: P(w₃|w₁,w₂)\n",
    "- Remember to include `<s>` and `</s>` tokens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for testing\n",
    "training_corpus = [\n",
    "    ['<s>', 'I', 'am', 'happy', '</s>'],\n",
    "    ['<s>', 'I', 'am', 'learning', '</s>'],\n",
    "    ['<s>', 'I', 'am', 'happy', 'because', 'I', 'am', 'learning', '</s>']\n",
    "]\n",
    "\n",
    "def build_ngram_counts(corpus, n):\n",
    "    \"\"\"\n",
    "    Build n-gram count dictionary from corpus.\n",
    "    \n",
    "    Args:\n",
    "        corpus: List of tokenized sentences\n",
    "        n: N-gram order (2 for bigram, 3 for trigram)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping n-grams (as tuples) to counts\n",
    "    \"\"\"\n",
    "    counts = defaultdict(int)\n",
    "    \n",
    "    for sentence in corpus:\n",
    "        # Slide window of size n over sentence\n",
    "        for i in range(len(sentence) - n + 1):\n",
    "            ngram = tuple(sentence[i:i+n])\n",
    "            counts[ngram] += 1\n",
    "    \n",
    "    return counts\n",
    "\n",
    "# Test the function\n",
    "bigram_counts = build_ngram_counts(training_corpus, 2)\n",
    "print(\"Bigram counts:\")\n",
    "for bigram, count in sorted(bigram_counts.items()):\n",
    "    print(f\"  {bigram}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Q1: N-gram Probability\n",
    "\n",
    "def calculate_ngram_probability(sentence, ngram_counts, n):\n",
    "    \"\"\"\n",
    "    Calculate probability of a sentence using n-gram model.\n",
    "    \n",
    "    Args:\n",
    "        sentence: List of tokens (should include <s> and </s>)\n",
    "        ngram_counts: Dictionary of n-gram counts\n",
    "        n: Order of n-gram (2 for bigram, 3 for trigram)\n",
    "    \n",
    "    Returns:\n",
    "        Probability of the sentence\n",
    "    \"\"\"\n",
    "    # Build (n-1)-gram counts for denominator\n",
    "    prefix_counts = defaultdict(int)\n",
    "    for ngram in ngram_counts:\n",
    "        prefix = ngram[:-1]  # All but last word\n",
    "        prefix_counts[prefix] += ngram_counts[ngram]\n",
    "    \n",
    "    probability = 1.0\n",
    "    \n",
    "    # Calculate probability using chain rule\n",
    "    for i in range(len(sentence) - n + 1):\n",
    "        ngram = tuple(sentence[i:i+n])\n",
    "        prefix = ngram[:-1]\n",
    "        \n",
    "        # P(wₙ|w₁...wₙ₋₁) = C(w₁...wₙ) / C(w₁...wₙ₋₁)\n",
    "        if prefix_counts[prefix] > 0:\n",
    "            prob = ngram_counts[ngram] / prefix_counts[prefix]\n",
    "        else:\n",
    "            prob = 0  # Unseen context\n",
    "        \n",
    "        probability *= prob\n",
    "    \n",
    "    return probability\n",
    "\n",
    "# Test with through-line example\n",
    "test_sentence = ['<s>', 'I', 'am', 'happy', '</s>']\n",
    "prob = calculate_ngram_probability(test_sentence, bigram_counts, 2)\n",
    "print(f\"\\nP({' '.join(test_sentence)}) = {prob}\")\n",
    "print(f\"This means: The model assigns {prob*100:.1f}% probability to this sentence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q2: Laplace (Add-1) Smoothing\n",
    "\n",
    "### The Zero Probability Problem\n",
    "\n",
    "**Scenario:** Training corpus has never seen \"am sad\"\n",
    "```python\n",
    "P(sad|am) = C(am sad) / C(am) = 0 / 4 = 0\n",
    "```\n",
    "\n",
    "**Consequence:** Entire sentence probability becomes 0!\n",
    "```python\n",
    "P(I am sad) = P(I) × P(am|I) × P(sad|am)\n",
    "            = 1.0 × 1.0 × 0 = 0  ❌\n",
    "```\n",
    "\n",
    "### Laplace Smoothing Solution\n",
    "\n",
    "**Idea:** Pretend we saw every possible n-gram at least once\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "P_laplace(wₙ|w₁...wₙ₋₁) = (C(w₁...wₙ) + 1) / (C(w₁...wₙ₋₁) + V)\n",
    "```\n",
    "where V = vocabulary size (number of unique words)\n",
    "\n",
    "### Visual Intuition: Redistributing Probability Mass\n",
    "\n",
    "**Before Smoothing (Bigrams after \"am\"):**\n",
    "```\n",
    "am happy:   ████████████ 50%\n",
    "am learning:████████████ 50%\n",
    "am sad:                   0%  ← Problem!\n",
    "am excited:               0%\n",
    "... (9,996 other words)   0%\n",
    "```\n",
    "\n",
    "**After Laplace Smoothing (V=10,000):**\n",
    "```\n",
    "am happy:   ██ 0.03%  (was 50%)\n",
    "am learning:██ 0.03%  (was 50%)\n",
    "am sad:     █  0.01%  (was 0%)  ✓\n",
    "am excited: █  0.01%  (was 0%)  ✓\n",
    "... each of 10,000 words gets 0.01%\n",
    "```\n",
    "\n",
    "### Trade-off\n",
    "✅ **Benefit:** No more zero probabilities  \n",
    "❌ **Cost:** Known n-grams lose most of their probability (50% → 0.03%)\n",
    "\n",
    "### Example Calculation\n",
    "```python\n",
    "# Original:\n",
    "C(am happy) = 2, C(am) = 4\n",
    "P(happy|am) = 2/4 = 0.5\n",
    "\n",
    "# With Laplace (V = 10,000):\n",
    "P_laplace(happy|am) = (2 + 1) / (4 + 10000) = 3/10004 ≈ 0.0003\n",
    "\n",
    "# Unseen n-gram:\n",
    "P_laplace(sad|am) = (0 + 1) / (4 + 10000) = 1/10004 ≈ 0.0001\n",
    "```\n",
    "\n",
    "### Tips\n",
    "- Always add V to denominator (not just V×n)\n",
    "- V should be vocabulary size, not number of n-grams\n",
    "- This method works for any n-gram order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Q2: Laplace Smoothing\n",
    "\n",
    "def calculate_laplace_probability(ngram, ngram_counts, vocab_size):\n",
    "    \"\"\"\n",
    "    Calculate n-gram probability with Laplace (add-1) smoothing.\n",
    "    \n",
    "    Args:\n",
    "        ngram: Tuple of words (e.g., ('am', 'happy'))\n",
    "        ngram_counts: Dictionary of n-gram counts\n",
    "        vocab_size: Number of unique words in vocabulary\n",
    "    \n",
    "    Returns:\n",
    "        Smoothed probability\n",
    "    \"\"\"\n",
    "    # Get n-gram count (0 if unseen)\n",
    "    ngram_count = ngram_counts.get(ngram, 0)\n",
    "    \n",
    "    # Get prefix count (all but last word)\n",
    "    prefix = ngram[:-1]\n",
    "    prefix_count = sum(count for ng, count in ngram_counts.items() \n",
    "                      if ng[:-1] == prefix)\n",
    "    \n",
    "    # Laplace formula: (C + 1) / (C_prefix + V)\n",
    "    probability = (ngram_count + 1) / (prefix_count + vocab_size)\n",
    "    \n",
    "    return probability\n",
    "\n",
    "# Build vocabulary from training corpus\n",
    "vocabulary = set()\n",
    "for sentence in training_corpus:\n",
    "    vocabulary.update(sentence)\n",
    "V = len(vocabulary)\n",
    "\n",
    "print(f\"Vocabulary size: {V}\\n\")\n",
    "\n",
    "# Test with known bigram\n",
    "known_bigram = ('am', 'happy')\n",
    "prob_known = calculate_laplace_probability(known_bigram, bigram_counts, V)\n",
    "print(f\"P_laplace{known_bigram} = {prob_known:.4f}\")\n",
    "\n",
    "# Test with unseen bigram\n",
    "unseen_bigram = ('am', 'sad')\n",
    "prob_unseen = calculate_laplace_probability(unseen_bigram, bigram_counts, V)\n",
    "print(f\"P_laplace{unseen_bigram} = {prob_unseen:.4f}\")\n",
    "\n",
    "print(f\"\\nNotice: Unseen n-gram gets non-zero probability!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q3: Perplexity\n",
    "\n",
    "### What is Perplexity?\n",
    "\n",
    "**Intuition:** Perplexity measures how \"confused\" or \"surprised\" your language model is by the test data.\n",
    "\n",
    "**Lower perplexity = Better model**\n",
    "\n",
    "### Mathematical Definition\n",
    "\n",
    "Perplexity is the **inverse probability** of the test set, normalized by the number of words:\n",
    "\n",
    "```\n",
    "PP(W) = P(w₁, w₂, ..., wₙ)^(-1/N)\n",
    "```\n",
    "\n",
    "where N is the number of words in test set.\n",
    "\n",
    "### Why Use Perplexity Instead of Probability?\n",
    "\n",
    "**Problem with raw probability:**\n",
    "```python\n",
    "P(\"I am happy\") = 0.5                    # 3 words\n",
    "P(\"I am happy because I am learning\") = 0.01  # 7 words\n",
    "# Longer sentences always have lower probability! Can't compare.\n",
    "```\n",
    "\n",
    "**Solution with perplexity:**\n",
    "```python\n",
    "PP(\"I am happy\") = 0.5^(-1/3) = 1.26\n",
    "PP(\"I am happy because I am learning\") = 0.01^(-1/7) = 1.86\n",
    "# Now we can compare! First sentence has lower perplexity → better modeled\n",
    "```\n",
    "\n",
    "### Interpretation Guide\n",
    "\n",
    "| Perplexity | Meaning | Example |\n",
    "|------------|---------|----------|\n",
    "| **1** | Perfect prediction | Model assigns P=1 to every word |\n",
    "| **10** | Choosing from ~10 words | Model is uncertain among 10 likely words |\n",
    "| **100** | Choosing from ~100 words | Model is confused, like random guessing from 100 options |\n",
    "| **1000+** | Very confused | Poor model, almost random |\n",
    "\n",
    "### Step-by-Step Example\n",
    "\n",
    "**Test sentence:** \"I am happy\"  \n",
    "**Bigram probabilities:** P(I|<s>)=1.0, P(am|I)=1.0, P(happy|am)=0.5, P(</s>|happy)=1.0\n",
    "\n",
    "**Step 1: Calculate sentence probability**\n",
    "```python\n",
    "P(sentence) = 1.0 × 1.0 × 0.5 × 1.0 = 0.5\n",
    "```\n",
    "\n",
    "**Step 2: Count words (excluding <s>, </s>)**\n",
    "```python\n",
    "N = 3  # [I, am, happy]\n",
    "```\n",
    "\n",
    "**Step 3: Calculate perplexity**\n",
    "```python\n",
    "PP = 0.5^(-1/3) = 1/(0.5^(1/3)) = 1/0.794 = 1.26\n",
    "```\n",
    "\n",
    "**Interpretation:** The model is about as confused as if it were choosing from 1.26 equally likely words. This is very good!\n",
    "\n",
    "### Connection to Cross-Entropy\n",
    "\n",
    "Perplexity is 2^(cross-entropy):\n",
    "```python\n",
    "Cross-Entropy = -1/N × Σ log₂ P(wᵢ|context)\n",
    "Perplexity = 2^(Cross-Entropy)\n",
    "```\n",
    "\n",
    "This connects to information theory: perplexity measures average branching factor.\n",
    "\n",
    "### Tips\n",
    "- Use `math.pow()` for fractional exponents\n",
    "- Typically count only content words (exclude `<s>` and `</s>`)\n",
    "- Lower perplexity = better language model\n",
    "- Compare perplexity only on the same test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Q3: Perplexity\n",
    "\n",
    "def calculate_perplexity(test_sentences, ngram_counts, n):\n",
    "    \"\"\"\n",
    "    Calculate perplexity of test set given n-gram model.\n",
    "    \n",
    "    Args:\n",
    "        test_sentences: List of tokenized test sentences\n",
    "        ngram_counts: Dictionary of n-gram counts from training\n",
    "        n: Order of n-gram\n",
    "    \n",
    "    Returns:\n",
    "        Perplexity value\n",
    "    \"\"\"\n",
    "    total_log_prob = 0\n",
    "    total_words = 0\n",
    "    \n",
    "    # Build prefix counts\n",
    "    prefix_counts = defaultdict(int)\n",
    "    for ngram in ngram_counts:\n",
    "        prefix = ngram[:-1]\n",
    "        prefix_counts[prefix] += ngram_counts[ngram]\n",
    "    \n",
    "    for sentence in test_sentences:\n",
    "        # Count words (excluding <s> and </s>)\n",
    "        words = [w for w in sentence if w not in ['<s>', '</s>']]\n",
    "        total_words += len(words)\n",
    "        \n",
    "        # Calculate log probability of sentence\n",
    "        for i in range(len(sentence) - n + 1):\n",
    "            ngram = tuple(sentence[i:i+n])\n",
    "            prefix = ngram[:-1]\n",
    "            \n",
    "            if prefix_counts[prefix] > 0:\n",
    "                prob = ngram_counts[ngram] / prefix_counts[prefix]\n",
    "            else:\n",
    "                prob = 1e-10  # Small probability for unseen\n",
    "            \n",
    "            # Avoid log(0)\n",
    "            if prob > 0:\n",
    "                total_log_prob += math.log(prob)\n",
    "            else:\n",
    "                total_log_prob += math.log(1e-10)\n",
    "    \n",
    "    # Perplexity = P(test)^(-1/N)\n",
    "    avg_log_prob = total_log_prob / total_words\n",
    "    perplexity = math.exp(-avg_log_prob)\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "# Test on training data (should be low perplexity)\n",
    "test_set = [['<s>', 'I', 'am', 'happy', '</s>']]\n",
    "pp = calculate_perplexity(test_set, bigram_counts, 2)\n",
    "print(f\"Perplexity on 'I am happy': {pp:.2f}\")\n",
    "print(f\"Interpretation: Model is as confused as choosing from {pp:.1f} equally likely words\")\n",
    "\n",
    "# Test on unseen data\n",
    "unseen_test = [['<s>', 'I', 'am', 'sad', '</s>']]\n",
    "pp_unseen = calculate_perplexity(unseen_test, bigram_counts, 2)\n",
    "print(f\"\\nPerplexity on 'I am sad': {pp_unseen:.2f}\")\n",
    "print(f\"Higher perplexity indicates unseen n-gram!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q4: Add-k Smoothing\n",
    "\n",
    "### The Laplace Problem\n",
    "\n",
    "Laplace (add-1) smoothing is too aggressive:\n",
    "```python\n",
    "# With V = 10,000\n",
    "P_laplace(happy|am) = (2 + 1) / (4 + 10000) = 0.0003\n",
    "P_laplace(sad|am)   = (0 + 1) / (4 + 10000) = 0.0001\n",
    "\n",
    "# Ratio is only 3:1, but original was infinite!\n",
    "# Unseen n-grams get too much probability\n",
    "```\n",
    "\n",
    "### Add-k Smoothing Solution\n",
    "\n",
    "**Idea:** Add a smaller value k instead of always adding 1\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "P_k(wₙ|w₁...wₙ₋₁) = (C(w₁...wₙ) + k) / (C(w₁...wₙ₋₁) + k×V)\n",
    "```\n",
    "\n",
    "### Comparing Different k Values\n",
    "\n",
    "Given: C(am happy) = 2, C(am) = 4, V = 10,000\n",
    "\n",
    "| k value | P(happy\\|am) | P(sad\\|am) | Ratio | Comment |\n",
    "|---------|--------------|------------|-------|----------|\n",
    "| **k = 0** | 2/4 = 0.5 | 0/4 = 0 | ∞ | No smoothing, zero prob problem |\n",
    "| **k = 0.01** | 2.01/4.1 = 0.490 | 0.01/4.1 = 0.0024 | 200:1 | Gentle smoothing |\n",
    "| **k = 0.1** | 2.1/14 = 0.150 | 0.1/14 = 0.0071 | 21:1 | Light smoothing |\n",
    "| **k = 1** | 3/10004 = 0.0003 | 1/10004 = 0.0001 | 3:1 | Laplace (too aggressive) |\n",
    "\n",
    "### Choosing k\n",
    "\n",
    "**Rule of thumb:**\n",
    "- k ∈ [0.01, 0.1] for most tasks\n",
    "- Smaller k = preserve original probabilities more\n",
    "- Larger k = redistribute more probability to unseen\n",
    "\n",
    "**Selection method:**\n",
    "1. Try different k values on validation set\n",
    "2. Choose k that gives lowest perplexity\n",
    "3. Different k works better for different corpus sizes\n",
    "\n",
    "### Visual Comparison\n",
    "\n",
    "**k = 0 (No smoothing):**\n",
    "```\n",
    "am happy:   ████████████████████████ 50%\n",
    "am learning:████████████████████████ 50%\n",
    "am sad:                               0%  ❌\n",
    "```\n",
    "\n",
    "**k = 0.1 (Add-k):**\n",
    "```\n",
    "am happy:   ████████████████ 15%\n",
    "am learning:████████████████ 15%\n",
    "am sad:     █ 0.7%  ✓ (small but non-zero)\n",
    "```\n",
    "\n",
    "**k = 1 (Laplace):**\n",
    "```\n",
    "am happy:   █ 0.03%\n",
    "am learning:█ 0.03%\n",
    "am sad:     █ 0.01%\n",
    "# All probabilities too small!\n",
    "```\n",
    "\n",
    "### Tips\n",
    "- Add k to numerator, k×V to denominator (not just V)\n",
    "- When k=1, this becomes Laplace smoothing\n",
    "- Smaller k usually works better for large vocabularies\n",
    "- Use validation set to tune k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Q4: Add-k Smoothing\n",
    "\n",
    "def calculate_add_k_probability(ngram, ngram_counts, vocab_size, k):\n",
    "    \"\"\"\n",
    "    Calculate n-gram probability with add-k smoothing.\n",
    "    \n",
    "    Args:\n",
    "        ngram: Tuple of words\n",
    "        ngram_counts: Dictionary of n-gram counts\n",
    "        vocab_size: Number of unique words\n",
    "        k: Smoothing parameter (0 < k ≤ 1)\n",
    "    \n",
    "    Returns:\n",
    "        Smoothed probability\n",
    "    \"\"\"\n",
    "    # Get n-gram count\n",
    "    ngram_count = ngram_counts.get(ngram, 0)\n",
    "    \n",
    "    # Get prefix count\n",
    "    prefix = ngram[:-1]\n",
    "    prefix_count = sum(count for ng, count in ngram_counts.items() \n",
    "                      if ng[:-1] == prefix)\n",
    "    \n",
    "    # Add-k formula: (C + k) / (C_prefix + k×V)\n",
    "    probability = (ngram_count + k) / (prefix_count + k * vocab_size)\n",
    "    \n",
    "    return probability\n",
    "\n",
    "# Compare different k values\n",
    "k_values = [0, 0.01, 0.1, 0.5, 1.0]\n",
    "test_bigrams = [('am', 'happy'), ('am', 'sad')]  # Known and unknown\n",
    "\n",
    "print(\"Comparing different k values:\\n\")\n",
    "print(f\"{'k':<6} | {'P(happy|am)':<12} | {'P(sad|am)':<12} | Ratio\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for k in k_values:\n",
    "    if k == 0:\n",
    "        # No smoothing - manual calculation\n",
    "        p_happy = 2/4  # C(am happy) / C(am)\n",
    "        p_sad = 0\n",
    "        ratio = \"∞\" if p_sad == 0 else f\"{p_happy/p_sad:.1f}:1\"\n",
    "    else:\n",
    "        p_happy = calculate_add_k_probability(test_bigrams[0], bigram_counts, V, k)\n",
    "        p_sad = calculate_add_k_probability(test_bigrams[1], bigram_counts, V, k)\n",
    "        ratio = f\"{p_happy/p_sad:.1f}:1\" if p_sad > 0 else \"∞\"\n",
    "    \n",
    "    print(f\"{k:<6} | {p_happy:<12.6f} | {p_sad:<12.6f} | {ratio}\")\n",
    "\n",
    "print(\"\\nObservation: Smaller k preserves the probability ratio better!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q5: Linear Interpolation\n",
    "\n",
    "### The Smoothing Hierarchy Problem\n",
    "\n",
    "Different n-gram orders have different strengths:\n",
    "\n",
    "| N-gram | Strength | Weakness | Example |\n",
    "|--------|----------|----------|----------|\n",
    "| **Trigram** | High precision (specific context) | Low coverage (sparse) | P(happy\\|I am) = 0.8 (but many unseen trigrams) |\n",
    "| **Bigram** | Medium precision | Medium coverage | P(happy\\|am) = 0.5 (more data, less specific) |\n",
    "| **Unigram** | Always available | No context | P(happy) = 0.3 (just word frequency) |\n",
    "\n",
    "### Interpolation Solution\n",
    "\n",
    "**Idea:** Combine all n-gram orders with learned weights!\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "P_interp(w₃|w₁,w₂) = λ₁ × P(w₃|w₁,w₂)    # Trigram\n",
    "                   + λ₂ × P(w₃|w₂)        # Bigram  \n",
    "                   + λ₃ × P(w₃)           # Unigram\n",
    "\n",
    "where λ₁ + λ₂ + λ₃ = 1 (weights sum to 1)\n",
    "```\n",
    "\n",
    "### Why This Works\n",
    "\n",
    "**Case 1: Seen trigram**\n",
    "```python\n",
    "# \"I am happy\" appears often in training\n",
    "P_trigram(happy|I,am) = 0.8    # High confidence\n",
    "P_bigram(happy|am) = 0.5       # Medium\n",
    "P_unigram(happy) = 0.3         # Low\n",
    "\n",
    "# Typical weights: λ₁=0.7, λ₂=0.2, λ₃=0.1\n",
    "P_interp = 0.7×0.8 + 0.2×0.5 + 0.1×0.3 = 0.69\n",
    "# Trigram dominates ✓\n",
    "```\n",
    "\n",
    "**Case 2: Unseen trigram, seen bigram**\n",
    "```python\n",
    "# \"I am excited\" never seen, but \"am excited\" exists\n",
    "P_trigram(excited|I,am) = 0.0   # No data\n",
    "P_bigram(excited|am) = 0.4      # Has data!\n",
    "P_unigram(excited) = 0.2        # Background\n",
    "\n",
    "P_interp = 0.7×0.0 + 0.2×0.4 + 0.1×0.2 = 0.10\n",
    "# Bigram saves us! ✓\n",
    "```\n",
    "\n",
    "**Case 3: Completely unseen words**\n",
    "```python\n",
    "# \"I am discombobulated\" - never seen anything\n",
    "P_trigram(discombobulated|I,am) = 0.0\n",
    "P_bigram(discombobulated|am) = 0.0\n",
    "P_unigram(discombobulated) = 0.0001  # Rare word\n",
    "\n",
    "P_interp = 0.7×0.0 + 0.2×0.0 + 0.1×0.0001 = 0.00001\n",
    "# Unigram provides fallback ✓\n",
    "```\n",
    "\n",
    "### Learning Lambda Weights\n",
    "\n",
    "**Method: Optimize on held-out validation set**\n",
    "\n",
    "```python\n",
    "# 1. Split data: 80% train, 10% validation, 10% test\n",
    "# 2. Build n-gram models on training set\n",
    "# 3. Try different λ combinations on validation set:\n",
    "\n",
    "for λ₁ in [0.5, 0.6, 0.7, 0.8]:\n",
    "    for λ₂ in [0.1, 0.2, 0.3]:\n",
    "        λ₃ = 1 - λ₁ - λ₂  # Ensure sum = 1\n",
    "        perplexity = evaluate_on_validation(λ₁, λ₂, λ₃)\n",
    "        \n",
    "# 4. Choose λ values with lowest perplexity\n",
    "# Typical result: λ₁ ≈ 0.7, λ₂ ≈ 0.2, λ₃ ≈ 0.1\n",
    "```\n",
    "\n",
    "### Comparison with Other Methods\n",
    "\n",
    "| Method | Unseen Trigram | Seen Bigram | Seen Unigram | Result |\n",
    "|--------|----------------|-------------|--------------|--------|\n",
    "| **No smoothing** | 0 | 0.4 | 0.2 | **0** (fails!) |\n",
    "| **Laplace k=1** | 0.0001 | 0.0004 | 0.002 | 0.0001 (all too small) |\n",
    "| **Add-k (k=0.1)** | 0.001 | 0.08 | 0.15 | 0.001 (better) |\n",
    "| **Back-off** | Uses bigram | 0.4 | 0.2 | 0.4 (uses bigram only) |\n",
    "| **Interpolation** | Blends all | 0.4 | 0.2 | **0.10** (combines strengths!) |\n",
    "\n",
    "### Tips\n",
    "- Weights must sum to 1: λ₁ + λ₂ + λ₃ = 1\n",
    "- Higher-order n-grams usually get larger weights (λ₁ > λ₂ > λ₃)\n",
    "- Can combine with smoothing: interpolate smoothed probabilities\n",
    "- Use EM algorithm for more sophisticated weight learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Q5: Linear Interpolation\n",
    "\n",
    "def calculate_interpolated_probability(trigram, unigram_counts, bigram_counts, \n",
    "                                       trigram_counts, lambda_1, lambda_2, lambda_3):\n",
    "    \"\"\"\n",
    "    Calculate probability using linear interpolation of trigram, bigram, unigram.\n",
    "    \n",
    "    Args:\n",
    "        trigram: Tuple of 3 words (w1, w2, w3)\n",
    "        unigram_counts: Dictionary of word counts\n",
    "        bigram_counts: Dictionary of bigram counts\n",
    "        trigram_counts: Dictionary of trigram counts\n",
    "        lambda_1, lambda_2, lambda_3: Weights (must sum to 1)\n",
    "    \n",
    "    Returns:\n",
    "        Interpolated probability\n",
    "    \"\"\"\n",
    "    w1, w2, w3 = trigram\n",
    "    \n",
    "    # Calculate trigram probability P(w3|w1,w2)\n",
    "    trigram_prefix = (w1, w2)\n",
    "    trigram_prefix_count = sum(c for ng, c in trigram_counts.items() \n",
    "                               if ng[:2] == trigram_prefix)\n",
    "    if trigram_prefix_count > 0:\n",
    "        p_trigram = trigram_counts.get(trigram, 0) / trigram_prefix_count\n",
    "    else:\n",
    "        p_trigram = 0\n",
    "    \n",
    "    # Calculate bigram probability P(w3|w2)\n",
    "    bigram = (w2, w3)\n",
    "    bigram_prefix = (w2,)\n",
    "    bigram_prefix_count = sum(c for ng, c in bigram_counts.items() \n",
    "                             if ng[:1] == bigram_prefix)\n",
    "    if bigram_prefix_count > 0:\n",
    "        p_bigram = bigram_counts.get(bigram, 0) / bigram_prefix_count\n",
    "    else:\n",
    "        p_bigram = 0\n",
    "    \n",
    "    # Calculate unigram probability P(w3)\n",
    "    total_words = sum(unigram_counts.values())\n",
    "    p_unigram = unigram_counts.get(w3, 0) / total_words\n",
    "    \n",
    "    # Linear interpolation\n",
    "    p_interpolated = (lambda_1 * p_trigram + \n",
    "                     lambda_2 * p_bigram + \n",
    "                     lambda_3 * p_unigram)\n",
    "    \n",
    "    return p_interpolated, p_trigram, p_bigram, p_unigram\n",
    "\n",
    "# Build unigram and trigram counts from our corpus\n",
    "unigram_counts = build_ngram_counts(training_corpus, 1)\n",
    "# Convert tuples to single words for unigrams\n",
    "unigram_counts = {k[0]: v for k, v in unigram_counts.items()}\n",
    "\n",
    "trigram_counts = build_ngram_counts(training_corpus, 3)\n",
    "\n",
    "# Test with typical weights\n",
    "lambda_1, lambda_2, lambda_3 = 0.7, 0.2, 0.1\n",
    "\n",
    "print(f\"Lambda weights: λ₁={lambda_1}, λ₂={lambda_2}, λ₃={lambda_3}\")\n",
    "print(f\"(Weights sum to {lambda_1 + lambda_2 + lambda_3})\\n\")\n",
    "\n",
    "# Test Case 1: Seen trigram\n",
    "test_trigram = ('I', 'am', 'happy')\n",
    "p_int, p_tri, p_bi, p_uni = calculate_interpolated_probability(\n",
    "    test_trigram, unigram_counts, bigram_counts, trigram_counts,\n",
    "    lambda_1, lambda_2, lambda_3\n",
    ")\n",
    "\n",
    "print(f\"Test: {test_trigram}\")\n",
    "print(f\"  P_trigram(happy|I,am) = {p_tri:.3f}\")\n",
    "print(f\"  P_bigram(happy|am)    = {p_bi:.3f}\")\n",
    "print(f\"  P_unigram(happy)      = {p_uni:.3f}\")\n",
    "print(f\"  P_interpolated        = {p_int:.3f}\")\n",
    "print(f\"  Calculation: {lambda_1}×{p_tri:.3f} + {lambda_2}×{p_bi:.3f} + {lambda_3}×{p_uni:.3f} = {p_int:.3f}\")\n",
    "\n",
    "# Test Case 2: Unseen trigram (but seen bigram)\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "test_trigram2 = ('am', 'learning', '</s>')\n",
    "p_int2, p_tri2, p_bi2, p_uni2 = calculate_interpolated_probability(\n",
    "    test_trigram2, unigram_counts, bigram_counts, trigram_counts,\n",
    "    lambda_1, lambda_2, lambda_3\n",
    ")\n",
    "\n",
    "print(f\"Test: {test_trigram2}\")\n",
    "print(f\"  P_trigram(</s>|am,learning) = {p_tri2:.3f}\")\n",
    "print(f\"  P_bigram(</s>|learning)     = {p_bi2:.3f}\")\n",
    "print(f\"  P_unigram(</s>)             = {p_uni2:.3f}\")\n",
    "print(f\"  P_interpolated              = {p_int2:.3f}\")\n",
    "print(f\"  Note: Bigram and unigram compensate for unseen trigram!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Putting It All Together: Complete Language Model Pipeline\n",
    "\n",
    "Now let's demonstrate the complete pipeline with all techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"COMPLETE N-GRAM LANGUAGE MODEL PIPELINE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n1. TRAINING CORPUS:\")\n",
    "for i, sent in enumerate(training_corpus, 1):\n",
    "    print(f\"   {i}. {' '.join(sent)}\")\n",
    "\n",
    "print(\"\\n2. N-GRAM COUNTS:\")\n",
    "print(f\"   Vocabulary size: {V}\")\n",
    "print(f\"   Unique bigrams: {len(bigram_counts)}\")\n",
    "print(f\"   Unique trigrams: {len(trigram_counts)}\")\n",
    "\n",
    "print(\"\\n3. PROBABILITY METHODS COMPARISON:\")\n",
    "test_sent = ['<s>', 'I', 'am', 'happy', '</s>']\n",
    "print(f\"   Test sentence: {' '.join(test_sent)}\\n\")\n",
    "\n",
    "# Method 1: Plain MLE\n",
    "prob_mle = calculate_ngram_probability(test_sent, bigram_counts, 2)\n",
    "print(f\"   Plain MLE:              P = {prob_mle:.4f}\")\n",
    "\n",
    "# Method 2: Laplace smoothing  \n",
    "prob_laplace = 1.0\n",
    "for i in range(len(test_sent) - 1):\n",
    "    bg = tuple(test_sent[i:i+2])\n",
    "    prob_laplace *= calculate_laplace_probability(bg, bigram_counts, V)\n",
    "print(f\"   Laplace (k=1):          P = {prob_laplace:.4f}\")\n",
    "\n",
    "# Method 3: Add-k smoothing\n",
    "prob_add_k = 1.0\n",
    "k = 0.1\n",
    "for i in range(len(test_sent) - 1):\n",
    "    bg = tuple(test_sent[i:i+2])\n",
    "    prob_add_k *= calculate_add_k_probability(bg, bigram_counts, V, k)\n",
    "print(f\"   Add-k (k=0.1):          P = {prob_add_k:.4f}\")\n",
    "\n",
    "print(\"\\n4. PERPLEXITY EVALUATION:\")\n",
    "pp_mle = calculate_perplexity([test_sent], bigram_counts, 2)\n",
    "print(f\"   Plain MLE perplexity:   {pp_mle:.2f}\")\n",
    "print(f\"   Interpretation: Model choosing from ~{pp_mle:.0f} equally likely words\")\n",
    "\n",
    "print(\"\\n5. INTERPOLATION (for trigrams):\")\n",
    "# Build sentence with trigram context\n",
    "trigram_example = ('I', 'am', 'happy')\n",
    "p_int, p_tri, p_bi, p_uni = calculate_interpolated_probability(\n",
    "    trigram_example, unigram_counts, bigram_counts, trigram_counts,\n",
    "    0.7, 0.2, 0.1\n",
    ")\n",
    "print(f\"   P_interp(happy|I,am) = {p_int:.4f}\")\n",
    "print(f\"   (Combined: {0.7}×{p_tri:.2f} + {0.2}×{p_bi:.2f} + {0.1}×{p_uni:.2f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY: Choose method based on your needs:\")\n",
    "print(\"  - Plain MLE: Fast, but fails on unseen n-grams\")\n",
    "print(\"  - Laplace: Simple, but too aggressive\")\n",
    "print(\"  - Add-k: Better balance, tune k on validation set\")\n",
    "print(\"  - Interpolation: Best performance, combines all orders\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps: From N-grams to Neural Language Models\n",
    "\n",
    "### Limitations of N-gram Models\n",
    "\n",
    "1. **Sparsity:** Most possible n-grams never appear in training\n",
    "2. **Fixed context:** Can't capture dependencies beyond n-1 words\n",
    "3. **No generalization:** \"cat sat\" and \"dog sat\" treated as completely different\n",
    "4. **Storage:** Need to store counts for millions of n-grams\n",
    "\n",
    "### Neural Language Models (Coming Soon!)\n",
    "\n",
    "**Key improvements:**\n",
    "- **Learned representations:** Words with similar meanings get similar vectors\n",
    "- **Generalization:** If model knows \"cat sat\", it can generalize to \"dog sat\"\n",
    "- **Variable context:** RNNs/Transformers can use arbitrarily long context\n",
    "- **Compact:** Store learned weights instead of all n-gram counts\n",
    "\n",
    "**But n-grams are still valuable:**\n",
    "- Baseline for comparison\n",
    "- Interpretable (can inspect exact counts)\n",
    "- No training required (just count!)\n",
    "- Still used in hybrid models\n",
    "\n",
    "You're now ready to understand how modern language models like GPT build on these foundations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
